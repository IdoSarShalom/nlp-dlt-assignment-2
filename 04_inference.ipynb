{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üèÜ Model Comparison: GRU vs BiLSTM\n",
        "\n",
        "This notebook compares the performance of GRU and BiLSTM models on the test dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìö Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìÇ Load Test Data\n",
        "\n",
        "First, we need to preprocess the test data using the preprocessing notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load preprocessed test data\n",
        "# NOTE: Before running this, make sure you've run preprocessing.ipynb with split='test'\n",
        "test_df = pd.read_pickle('./data/test_preprocessed.pkl')\n",
        "\n",
        "print(f\"Test data shape: {test_df.shape}\")\n",
        "print(f\"Columns: {test_df.columns.tolist()}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "print(test_df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Prepare Data\n",
        "\n",
        "Split the data into features (X) and labels (y)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare X and y\n",
        "X_test = test_df['Text']\n",
        "y_test = test_df['Label']\n",
        "\n",
        "print(f\"Test samples: {len(X_test)}\")\n",
        "print(f\"\\nLabel distribution in test set:\")\n",
        "print(y_test.value_counts().sort_index())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîÑ Load Models and Tokenizers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load GRU model and assets\n",
        "print(\"Loading GRU model...\")\n",
        "gru_model = load_model('./data/gru/gru_model.keras')\n",
        "\n",
        "with open('./data/gru/gru_tokenizer.pkl', 'rb') as f:\n",
        "    gru_tokenizer = pickle.load(f)\n",
        "\n",
        "with open('./data/gru/gru_metadata.pkl', 'rb') as f:\n",
        "    gru_metadata = pickle.load(f)\n",
        "\n",
        "print(f\"‚úÖ GRU model loaded successfully\")\n",
        "print(f\"   Training validation accuracy: {gru_metadata['val_accuracy']:.4f}\")\n",
        "\n",
        "# Load BiLSTM model and assets\n",
        "print(\"\\nLoading BiLSTM model...\")\n",
        "lstm_model = load_model('./data/lstm/lstm_model.keras')\n",
        "\n",
        "with open('./data/lstm/lstm_tokenizer.pkl', 'rb') as f:\n",
        "    lstm_tokenizer = pickle.load(f)\n",
        "\n",
        "with open('./data/lstm/lstm_metadata.pkl', 'rb') as f:\n",
        "    lstm_metadata = pickle.load(f)\n",
        "\n",
        "print(f\"‚úÖ BiLSTM model loaded successfully\")\n",
        "print(f\"   Training validation accuracy: {lstm_metadata['val_accuracy']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üî† Prepare Test Data for GRU Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tokenize and pad for GRU model\n",
        "X_test_gru_sequences = gru_tokenizer.texts_to_sequences(X_test)\n",
        "X_test_gru_padded = pad_sequences(X_test_gru_sequences, \n",
        "                                   maxlen=gru_metadata['maxlen'], \n",
        "                                   padding='post')\n",
        "\n",
        "print(f\"GRU test data shape: {X_test_gru_padded.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üî† Prepare Test Data for BiLSTM Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tokenize and pad for BiLSTM model\n",
        "X_test_lstm_sequences = lstm_tokenizer.texts_to_sequences(X_test)\n",
        "X_test_lstm_padded = pad_sequences(X_test_lstm_sequences, \n",
        "                                    maxlen=lstm_metadata['maxlen'], \n",
        "                                    padding='post')\n",
        "\n",
        "print(f\"BiLSTM test data shape: {X_test_lstm_padded.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Evaluate GRU Model on Test Set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate GRU model\n",
        "gru_test_loss, gru_test_accuracy = gru_model.evaluate(X_test_gru_padded, y_test, verbose=0)\n",
        "\n",
        "# Get predictions\n",
        "y_pred_gru = gru_model.predict(X_test_gru_padded, verbose=0)\n",
        "y_pred_gru = np.argmax(y_pred_gru, axis=1)\n",
        "\n",
        "print(f\"GRU Model Test Results:\")\n",
        "print(f\"  Test Loss: {gru_test_loss:.4f}\")\n",
        "print(f\"  Test Accuracy: {gru_test_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Evaluate BiLSTM Model on Test Set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate BiLSTM model\n",
        "lstm_test_loss, lstm_test_accuracy = lstm_model.evaluate(X_test_lstm_padded, y_test, verbose=0)\n",
        "\n",
        "# Get predictions\n",
        "y_pred_lstm = lstm_model.predict(X_test_lstm_padded, verbose=0)\n",
        "y_pred_lstm = np.argmax(y_pred_lstm, axis=1)\n",
        "\n",
        "print(f\"BiLSTM Model Test Results:\")\n",
        "print(f\"  Test Loss: {lstm_test_loss:.4f}\")\n",
        "print(f\"  Test Accuracy: {lstm_test_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Compare Model Performance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comparison dataframe\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Model': ['GRU', 'BiLSTM'],\n",
        "    'Test Accuracy': [gru_test_accuracy, lstm_test_accuracy],\n",
        "    'Test Loss': [gru_test_loss, lstm_test_loss]\n",
        "})\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MODEL COMPARISON SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(comparison_df.to_string(index=False))\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Visualize comparison\n",
        "fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Accuracy comparison\n",
        "axs[0].bar(comparison_df['Model'], comparison_df['Test Accuracy'], \n",
        "           color=['#3498db', '#2ecc71'], edgecolor='black', linewidth=2)\n",
        "axs[0].set_ylabel('Accuracy')\n",
        "axs[0].set_title('Test Accuracy Comparison')\n",
        "axs[0].set_ylim([0, 1])\n",
        "axs[0].grid(True, alpha=0.3)\n",
        "for i, v in enumerate(comparison_df['Test Accuracy']):\n",
        "    axs[0].text(i, v + 0.02, f'{v:.4f}', ha='center', fontweight='bold')\n",
        "\n",
        "# Loss comparison\n",
        "axs[1].bar(comparison_df['Model'], comparison_df['Test Loss'], \n",
        "           color=['#e74c3c', '#f39c12'], edgecolor='black', linewidth=2)\n",
        "axs[1].set_ylabel('Loss')\n",
        "axs[1].set_title('Test Loss Comparison')\n",
        "axs[1].grid(True, alpha=0.3)\n",
        "for i, v in enumerate(comparison_df['Test Loss']):\n",
        "    axs[1].text(i, v + 0.01, f'{v:.4f}', ha='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Confusion Matrices - Side by Side\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create confusion matrices\n",
        "cm_gru = confusion_matrix(y_test, y_pred_gru)\n",
        "cm_lstm = confusion_matrix(y_test, y_pred_lstm)\n",
        "\n",
        "# Plot confusion matrices side by side\n",
        "fig, axs = plt.subplots(1, 2, figsize=(18, 7))\n",
        "\n",
        "# GRU confusion matrix\n",
        "sns.heatmap(cm_gru, annot=True, fmt='d', cmap='Blues', ax=axs[0],\n",
        "            xticklabels=emotion_labels, yticklabels=emotion_labels)\n",
        "axs[0].set_xlabel('Predicted labels')\n",
        "axs[0].set_ylabel('True labels')\n",
        "axs[0].set_title(f'GRU Model - Confusion Matrix\\nAccuracy: {gru_test_accuracy:.4f}')\n",
        "\n",
        "# BiLSTM confusion matrix\n",
        "sns.heatmap(cm_lstm, annot=True, fmt='d', cmap='Greens', ax=axs[1],\n",
        "            xticklabels=emotion_labels, yticklabels=emotion_labels)\n",
        "axs[1].set_xlabel('Predicted labels')\n",
        "axs[1].set_ylabel('True labels')\n",
        "axs[1].set_title(f'BiLSTM Model - Confusion Matrix\\nAccuracy: {lstm_test_accuracy:.4f}')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù Classification Reports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GRU Classification Report\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"GRU MODEL - CLASSIFICATION REPORT\")\n",
        "print(\"=\"*60)\n",
        "print(classification_report(y_test, y_pred_gru, target_names=emotion_labels))\n",
        "\n",
        "# BiLSTM Classification Report\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"BiLSTM MODEL - CLASSIFICATION REPORT\")\n",
        "print(\"=\"*60)\n",
        "print(classification_report(y_test, y_pred_lstm, target_names=emotion_labels))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Per-Class Performance Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get classification reports as dictionaries\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "gru_precision, gru_recall, gru_f1, _ = precision_recall_fscore_support(\n",
        "    y_test, y_pred_gru, average=None, labels=list(range(6))\n",
        ")\n",
        "\n",
        "lstm_precision, lstm_recall, lstm_f1, _ = precision_recall_fscore_support(\n",
        "    y_test, y_pred_lstm, average=None, labels=list(range(6))\n",
        ")\n",
        "\n",
        "# Create comparison dataframe\n",
        "emotion_comparison = pd.DataFrame({\n",
        "    'Emotion': emotion_labels,\n",
        "    'GRU Precision': gru_precision,\n",
        "    'LSTM Precision': lstm_precision,\n",
        "    'GRU Recall': gru_recall,\n",
        "    'LSTM Recall': lstm_recall,\n",
        "    'GRU F1': gru_f1,\n",
        "    'LSTM F1': lstm_f1\n",
        "})\n",
        "\n",
        "print(\"\\nPer-Class Performance Comparison:\")\n",
        "print(emotion_comparison.to_string(index=False))\n",
        "\n",
        "# Visualize F1-scores comparison\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "x = np.arange(len(emotion_labels))\n",
        "width = 0.35\n",
        "\n",
        "bars1 = ax.bar(x - width/2, gru_f1, width, label='GRU', color='#3498db', edgecolor='black')\n",
        "bars2 = ax.bar(x + width/2, lstm_f1, width, label='BiLSTM', color='#2ecc71', edgecolor='black')\n",
        "\n",
        "ax.set_xlabel('Emotion')\n",
        "ax.set_ylabel('F1-Score')\n",
        "ax.set_title('F1-Score Comparison by Emotion Class')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(emotion_labels)\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Add value labels on bars\n",
        "for bars in [bars1, bars2]:\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                f'{height:.3f}', ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üèÜ Final Verdict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Determine winner\n",
        "if gru_test_accuracy > lstm_test_accuracy:\n",
        "    winner = \"GRU\"\n",
        "    winner_acc = gru_test_accuracy\n",
        "    diff = gru_test_accuracy - lstm_test_accuracy\n",
        "elif lstm_test_accuracy > gru_test_accuracy:\n",
        "    winner = \"BiLSTM\"\n",
        "    winner_acc = lstm_test_accuracy\n",
        "    diff = lstm_test_accuracy - gru_test_accuracy\n",
        "else:\n",
        "    winner = \"TIE\"\n",
        "    winner_acc = gru_test_accuracy\n",
        "    diff = 0\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üèÜ FINAL VERDICT\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if winner == \"TIE\":\n",
        "    print(\"Both models performed equally well!\")\n",
        "else:\n",
        "    print(f\"Winner: {winner} Model\")\n",
        "    print(f\"  Accuracy: {winner_acc:.4f}\")\n",
        "    print(f\"  Margin: +{diff:.4f} ({diff*100:.2f}%)\")\n",
        "\n",
        "print(\"\\nüìä Summary:\")\n",
        "print(f\"  GRU Model:    {gru_test_accuracy:.4f} accuracy\")\n",
        "print(f\"  BiLSTM Model: {lstm_test_accuracy:.4f} accuracy\")\n",
        "print(\"=\"*60)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
