{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ü§ó RoBERTa Model Training: Emotion Classification\n",
        "\n",
        "This notebook fine-tunes a **RoBERTa (Robustly Optimized BERT Pretraining Approach)** model on emotion data for Part B of the assignment.\n",
        "\n",
        "**Model:** RoBERTa-base - 125M parameters, an optimized variant of BERT with improved training procedure.\n",
        "\n",
        "**Why RoBERTa?** RoBERTa builds upon BERT with key improvements: longer training, larger batches, dynamic masking, and removal of Next Sentence Prediction. It consistently outperforms BERT on most NLP benchmarks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìö Import Libraries\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Install Dependencies (Run on Colab)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if running on Google Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "\n",
        "# Install dependencies only on Colab\n",
        "if IN_COLAB:\n",
        "    print(\"Running on Google Colab - Installing dependencies...\")\n",
        "    %pip install -q transformers==4.36.0 torch==2.1.0 datasets==2.16.0 accelerate==0.25.0\n",
        "    %pip install -q pandas==2.3.3 numpy==2.2.5 scikit-learn==1.7.2 matplotlib==3.10.6 seaborn==0.13.2\n",
        "    print(\"Dependencies installed successfully!\")\n",
        "else:\n",
        "    print(\"Running locally - Using local dependencies\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üñ•Ô∏è GPU Configuration Check\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability for PyTorch\n",
        "import torch\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"üîç CHECKING GPU AVAILABILITY\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"\\n‚úÖ GPU IS AVAILABLE - Training will use GPU acceleration!\")\n",
        "    print(f\"   GPU Device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Number of GPUs: {torch.cuda.device_count()}\")\n",
        "    print(f\"   CUDA Version: {torch.version.cuda}\")\n",
        "    print(f\"\\n   üöÄ Expected speedup: 10-20x faster than CPU!\")\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    print(f\"\\n‚ö†Ô∏è  NO GPU DETECTED - Training will use CPU only\")\n",
        "    print(f\"   Note: RoBERTa training on CPU is very slow\")\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Transformers imports\n",
        "from transformers import (\n",
        "    RobertaTokenizer, \n",
        "    RobertaForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "from datasets import Dataset\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import random\n",
        "import os\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "SEED = 42\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "**üìù Important Note on GPU Usage:**\n",
        "\n",
        "PyTorch and Transformers **automatically use your GPU** when available. The Trainer API handles device placement automatically.\n",
        "\n",
        "- If GPU was detected above ‚úÖ, all training will run on GPU\n",
        "- RoBERTa training is significantly faster on GPU (10-20x speedup)\n",
        "- Monitor GPU usage: `watch -n 1 nvidia-smi`\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìÇ Load Data\n",
        "\n",
        "**Note on Preprocessing:** This notebook uses the preprocessed data from `01_preprocessing.ipynb` (same as Part A models) for fair comparison. While transformers typically perform better with raw text, using consistent preprocessing across all models ensures a valid comparative analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load preprocessed data (following the same flow as GRU/LSTM notebooks)\n",
        "# Note: Transformers typically work with raw text, but for fair comparison\n",
        "# we use the same preprocessed data as Part A models\n",
        "train_df = pd.read_pickle('./data/train_preprocessed.pkl')\n",
        "val_df = pd.read_pickle('./data/validation_preprocessed.pkl')\n",
        "\n",
        "print(f\"Training data shape: {train_df.shape}\")\n",
        "print(f\"Validation data shape: {val_df.shape}\")\n",
        "print(f\"\\nColumns: {train_df.columns.tolist()}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "print(train_df.head())\n",
        "\n",
        "# Emotion labels\n",
        "emotion_labels = ['Sadness', 'Joy', 'Love', 'Anger', 'Fear', 'Surprise']\n",
        "num_labels = len(emotion_labels)\n",
        "\n",
        "print(f\"\\nüìä Number of classes: {num_labels}\")\n",
        "print(f\"Labels: {emotion_labels}\")\n",
        "print(f\"\\nLabel distribution in training set:\")\n",
        "print(train_df['Label'].value_counts().sort_index())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üî† RoBERTa Tokenization\n",
        "\n",
        "RoBERTa uses Byte-Pair Encoding (BPE) tokenization without Next Sentence Prediction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load RoBERTa tokenizer\n",
        "model_name = 'roberta-base'\n",
        "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
        "\n",
        "print(f\"‚úÖ Loaded tokenizer: {model_name}\")\n",
        "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
        "\n",
        "# Find optimal max length using preprocessed text\n",
        "train_lengths = train_df['Text'].apply(lambda x: len(tokenizer.encode(x)))\n",
        "print(f\"\\nüìä Token length statistics:\")\n",
        "print(f\"   Mean: {train_lengths.mean():.1f}\")\n",
        "print(f\"   Median: {train_lengths.median():.1f}\")\n",
        "print(f\"   95th percentile: {train_lengths.quantile(0.95):.1f}\")\n",
        "print(f\"   Max: {train_lengths.max()}\")\n",
        "\n",
        "# Use 128 as max_length (covers ~99% of samples while being efficient)\n",
        "max_length = 128\n",
        "print(f\"\\n‚úÖ Using max_length={max_length} for tokenization\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tokenize function for datasets\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples['Text'],\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "# Create HuggingFace datasets from preprocessed data\n",
        "# Note: Column names are 'Text' and 'Label' (capitalized) in preprocessed files\n",
        "train_dataset = Dataset.from_pandas(train_df[['Text', 'Label']].rename(columns={'Label': 'label'}))\n",
        "val_dataset = Dataset.from_pandas(val_df[['Text', 'Label']].rename(columns={'Label': 'label'}))\n",
        "\n",
        "# Tokenize datasets\n",
        "print(\"Tokenizing training data...\")\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "print(\"Tokenizing validation data...\")\n",
        "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Set format for PyTorch\n",
        "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "\n",
        "print(f\"\\n‚úÖ Datasets prepared:\")\n",
        "print(f\"   Training samples: {len(train_dataset)}\")\n",
        "print(f\"   Validation samples: {len(val_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üèóÔ∏è Load Pre-trained RoBERTa Model\n",
        "\n",
        "We'll fine-tune RoBERTa for sequence classification with 6 emotion labels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load pre-trained RoBERTa model for sequence classification\n",
        "model = RobertaForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=num_labels,\n",
        "    problem_type=\"single_label_classification\"\n",
        ")\n",
        "\n",
        "# Move model to GPU if available\n",
        "model.to(device)\n",
        "\n",
        "print(f\"‚úÖ Loaded pre-trained RoBERTa model: {model_name}\")\n",
        "print(f\"   Number of parameters: {model.num_parameters():,}\")\n",
        "print(f\"   Model device: {next(model.parameters()).device}\")\n",
        "\n",
        "# Model architecture summary\n",
        "print(f\"\\nüìã Model Architecture:\")\n",
        "print(f\"   RoBERTa layers: {model.config.num_hidden_layers}\")\n",
        "print(f\"   Hidden size: {model.config.hidden_size}\")\n",
        "print(f\"   Attention heads: {model.config.num_attention_heads}\")\n",
        "print(f\"   Vocabulary size: {model.config.vocab_size}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚öôÔ∏è Configure Training Arguments\n",
        "\n",
        "Set up hyperparameters for fine-tuning RoBERTa.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define output directory\n",
        "output_dir = './data/roberta'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Training arguments optimized for emotion classification\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    \n",
        "    # Training hyperparameters\n",
        "    num_train_epochs=5,              # 3-5 epochs typical for fine-tuning\n",
        "    per_device_train_batch_size=16,  # Adjust based on GPU memory\n",
        "    per_device_eval_batch_size=32,   # Can be larger for evaluation\n",
        "    learning_rate=2e-5,              # Standard for RoBERTa fine-tuning\n",
        "    weight_decay=0.01,               # L2 regularization\n",
        "    warmup_steps=500,                # Gradual learning rate warmup\n",
        "    \n",
        "    # Evaluation and saving\n",
        "    evaluation_strategy=\"epoch\",     # Evaluate after each epoch\n",
        "    save_strategy=\"epoch\",           # Save checkpoint after each epoch\n",
        "    save_total_limit=2,              # Keep only best 2 checkpoints\n",
        "    load_best_model_at_end=True,     # Load best model after training\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    greater_is_better=True,\n",
        "    \n",
        "    # Logging\n",
        "    logging_dir=f'{output_dir}/logs',\n",
        "    logging_steps=100,\n",
        "    logging_strategy=\"steps\",\n",
        "    \n",
        "    # Performance\n",
        "    fp16=torch.cuda.is_available(),  # Mixed precision training on GPU\n",
        "    dataloader_num_workers=2,\n",
        "    \n",
        "    # Reproducibility\n",
        "    seed=SEED,\n",
        "    \n",
        "    # Other\n",
        "    report_to=\"none\",                # Disable wandb/tensorboard\n",
        "    push_to_hub=False,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Training arguments configured:\")\n",
        "print(f\"   Epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"   Batch size (train): {training_args.per_device_train_batch_size}\")\n",
        "print(f\"   Batch size (eval): {training_args.per_device_eval_batch_size}\")\n",
        "print(f\"   Learning rate: {training_args.learning_rate}\")\n",
        "print(f\"   Weight decay: {training_args.weight_decay}\")\n",
        "print(f\"   Warmup steps: {training_args.warmup_steps}\")\n",
        "print(f\"   FP16 training: {training_args.fp16}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute metrics function for Trainer\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    return {'accuracy': accuracy}\n",
        "\n",
        "print(\"‚úÖ Metrics function defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Initialize Trainer and Start Fine-tuning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Trainer initialized\")\n",
        "print(\"\\nüöÄ Starting RoBERTa fine-tuning...\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "train_result = trainer.train()\n",
        "training_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\n‚úÖ Training completed!\")\n",
        "print(f\"   Training time: {training_time/60:.2f} minutes\")\n",
        "print(f\"   Best checkpoint: {trainer.state.best_model_checkpoint}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Visualize Training Progress\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract training history\n",
        "log_history = trainer.state.log_history\n",
        "\n",
        "# Separate train and eval logs\n",
        "train_logs = [log for log in log_history if 'loss' in log and 'eval_loss' not in log]\n",
        "eval_logs = [log for log in log_history if 'eval_loss' in log]\n",
        "\n",
        "# Extract metrics\n",
        "train_loss = [log['loss'] for log in train_logs]\n",
        "train_steps = [log['step'] for log in train_logs]\n",
        "\n",
        "eval_loss = [log['eval_loss'] for log in eval_logs]\n",
        "eval_accuracy = [log['eval_accuracy'] for log in eval_logs]\n",
        "eval_epochs = [log['epoch'] for log in eval_logs]\n",
        "\n",
        "# Plot training history\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Plot loss\n",
        "ax1 = axes[0]\n",
        "ax1_twin = ax1.twiny()\n",
        "ax1.plot(train_steps, train_loss, label='Training Loss', alpha=0.7, color='blue')\n",
        "ax1_twin.plot(eval_epochs, eval_loss, 'o-', label='Validation Loss', color='red', markersize=8)\n",
        "ax1.set_xlabel('Training Steps')\n",
        "ax1_twin.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
        "ax1.legend(loc='upper left')\n",
        "ax1_twin.legend(loc='upper right')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot accuracy\n",
        "axes[1].plot(eval_epochs, eval_accuracy, 'o-', marker='s', markersize=8, color='green', linewidth=2)\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Accuracy')\n",
        "axes[1].set_title('Validation Accuracy', fontsize=14, fontweight='bold')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "axes[1].set_ylim([min(eval_accuracy) - 0.02, 1.0])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nüìä Final Training Metrics:\")\n",
        "print(f\"   Best Validation Accuracy: {max(eval_accuracy):.4f}\")\n",
        "print(f\"   Final Validation Loss: {eval_loss[-1]:.4f}\")\n",
        "print(f\"   Training completed in: {training_time/60:.2f} minutes\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìà Evaluate Final Model Performance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on validation set\n",
        "eval_results = trainer.evaluate()\n",
        "\n",
        "print(f\"\\nüìä Final Model Performance:\")\n",
        "print(f\"  Validation Loss: {eval_results['eval_loss']:.4f}\")\n",
        "print(f\"  Validation Accuracy: {eval_results['eval_accuracy']:.4f}\")\n",
        "\n",
        "# Generate predictions for confusion matrix\n",
        "predictions = trainer.predict(val_dataset)\n",
        "y_pred = np.argmax(predictions.predictions, axis=-1)\n",
        "y_true = predictions.label_ids\n",
        "\n",
        "print(f\"\\n‚úÖ Predictions generated for {len(y_pred)} samples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Confusion Matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', \n",
        "            xticklabels=emotion_labels,\n",
        "            yticklabels=emotion_labels)\n",
        "plt.title('Confusion Matrix - RoBERTa Model', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù Classification Report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate classification report\n",
        "report = classification_report(y_true, y_pred, target_names=emotion_labels)\n",
        "print(\"\\nüìù Classification Report:\")\n",
        "print(\"=\" * 60)\n",
        "print(report)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Model Statistics Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate model size\n",
        "model_size_mb = sum(p.numel() * p.element_size() for p in model.parameters()) / (1024 * 1024)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üìä RoBERTa MODEL STATISTICS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Model Name:              {model_name}\")\n",
        "print(f\"Total Parameters:        {model.num_parameters():,}\")\n",
        "print(f\"Model Size:              {model_size_mb:.2f} MB\")\n",
        "print(f\"Training Time:           {training_time/60:.2f} minutes\")\n",
        "print(f\"Validation Accuracy:     {eval_results['eval_accuracy']:.4f} ({eval_results['eval_accuracy']*100:.2f}%)\")\n",
        "print(f\"Validation Loss:         {eval_results['eval_loss']:.4f}\")\n",
        "print(f\"Max Sequence Length:     {max_length}\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üíæ Save Final Model and Tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the final model and tokenizer\n",
        "final_model_dir = './data/roberta/final_model'\n",
        "os.makedirs(final_model_dir, exist_ok=True)\n",
        "\n",
        "# Save model\n",
        "trainer.save_model(final_model_dir)\n",
        "print(f\"‚úÖ Model saved to: {final_model_dir}\")\n",
        "\n",
        "# Save tokenizer\n",
        "tokenizer.save_pretrained(final_model_dir)\n",
        "print(f\"‚úÖ Tokenizer saved to: {final_model_dir}\")\n",
        "\n",
        "# Save metadata\n",
        "metadata = {\n",
        "    'model_name': model_name,\n",
        "    'num_parameters': model.num_parameters(),\n",
        "    'model_size_mb': model_size_mb,\n",
        "    'max_length': max_length,\n",
        "    'num_labels': num_labels,\n",
        "    'emotion_labels': emotion_labels,\n",
        "    'val_accuracy': eval_results['eval_accuracy'],\n",
        "    'val_loss': eval_results['eval_loss'],\n",
        "    'training_time_minutes': training_time/60,\n",
        "    'training_args': {\n",
        "        'num_epochs': training_args.num_train_epochs,\n",
        "        'batch_size': training_args.per_device_train_batch_size,\n",
        "        'learning_rate': training_args.learning_rate,\n",
        "        'weight_decay': training_args.weight_decay,\n",
        "        'warmup_steps': training_args.warmup_steps\n",
        "    }\n",
        "}\n",
        "\n",
        "metadata_path = os.path.join(output_dir, 'roberta_metadata.pkl')\n",
        "with open(metadata_path, 'wb') as f:\n",
        "    pickle.dump(metadata, f)\n",
        "print(f\"‚úÖ Metadata saved to: {metadata_path}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"‚úÖ All files saved successfully!\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nFinal Model Performance:\")\n",
        "print(f\"  Validation Accuracy: {eval_results['eval_accuracy']:.4f}\")\n",
        "print(f\"  Validation Loss: {eval_results['eval_loss']:.4f}\")\n",
        "print(f\"  Model Size: {model_size_mb:.2f} MB\")\n",
        "print(f\"  Parameters: {model.num_parameters():,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
