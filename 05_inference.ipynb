{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üèÜ Transformer Models Comparison: BERT vs RoBERTa vs ELECTRA\n",
        "\n",
        "This notebook compares the performance of three state-of-the-art transformer models on the emotion classification test dataset:\n",
        "- **BERT** (bert-base-uncased) - 110M parameters\n",
        "- **RoBERTa** (roberta-base) - 125M parameters  \n",
        "- **ELECTRA** (electra-base-discriminator) - 110M parameters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìö Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_recall_fscore_support\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Transformer imports\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìÇ Load Test Data\n",
        "\n",
        "**Note:** Make sure you've run `01_preprocessing.ipynb` with `split='test'` to generate `test_preprocessed.pkl`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load preprocessed test data\n",
        "test_df = pd.read_pickle('./data/test_preprocessed.pkl')\n",
        "\n",
        "print(f\"Test data shape: {test_df.shape}\")\n",
        "print(f\"Columns: {test_df.columns.tolist()}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "print(test_df.head())\n",
        "\n",
        "# Prepare data\n",
        "X_test = test_df['Text']\n",
        "y_test = test_df['Label']\n",
        "\n",
        "# Emotion labels\n",
        "emotion_labels = ['Sadness', 'Joy', 'Love', 'Anger', 'Fear', 'Surprise']\n",
        "\n",
        "print(f\"\\nTest samples: {len(X_test)}\")\n",
        "print(f\"\\nLabel distribution in test set:\")\n",
        "print(y_test.value_counts().sort_index())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîÑ Load Transformer Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model configurations\n",
        "models_config = {\n",
        "    'BERT': './data/bert/final_model',\n",
        "    'RoBERTa': './data/roberta/final_model',\n",
        "    'ELECTRA': './data/electra/final_model'\n",
        "}\n",
        "\n",
        "# Load all models and tokenizers\n",
        "models = {}\n",
        "tokenizers = {}\n",
        "metadata = {}\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\\n\")\n",
        "\n",
        "for model_name, model_path in models_config.items():\n",
        "    print(f\"Loading {model_name} model...\")\n",
        "    \n",
        "    # Load model\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    models[model_name] = model\n",
        "    \n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    tokenizers[model_name] = tokenizer\n",
        "    \n",
        "    # Load metadata\n",
        "    metadata_path = f'./data/{model_name.lower()}/{model_name.lower()}_metadata.pkl'\n",
        "    with open(metadata_path, 'rb') as f:\n",
        "        metadata[model_name] = pickle.load(f)\n",
        "    \n",
        "    print(f\"   ‚úÖ {model_name} loaded successfully\")\n",
        "    print(f\"      Training validation accuracy: {metadata[model_name]['val_accuracy']:.4f}\")\n",
        "    print(f\"      Parameters: {metadata[model_name]['num_parameters']:,}\")\n",
        "    print(f\"      Size: {metadata[model_name]['model_size_mb']:.2f} MB\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Evaluate Models on Test Set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to evaluate a model\n",
        "def evaluate_model(model, tokenizer, texts, labels, model_name, max_length=128, batch_size=32):\n",
        "    \"\"\"Evaluate a transformer model on test data\"\"\"\n",
        "    \n",
        "    print(f\"Evaluating {model_name}...\")\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Tokenize data\n",
        "    encodings = tokenizer(\n",
        "        texts.tolist(),\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    \n",
        "    # Create dataset\n",
        "    dataset = torch.utils.data.TensorDataset(\n",
        "        encodings['input_ids'],\n",
        "        encodings['attention_mask']\n",
        "    )\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n",
        "    \n",
        "    # Get predictions\n",
        "    all_predictions = []\n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids, attention_mask = [b.to(device) for b in batch]\n",
        "            \n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "            all_predictions.extend(predictions.cpu().numpy())\n",
        "    \n",
        "    inference_time = time.time() - start_time\n",
        "    \n",
        "    # Calculate metrics\n",
        "    y_pred = np.array(all_predictions)\n",
        "    accuracy = accuracy_score(labels, y_pred)\n",
        "    \n",
        "    print(f\"   ‚úÖ {model_name} evaluation complete\")\n",
        "    print(f\"      Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"      Inference time: {inference_time:.2f} seconds\\n\")\n",
        "    \n",
        "    return {\n",
        "        'predictions': y_pred,\n",
        "        'accuracy': accuracy,\n",
        "        'inference_time': inference_time\n",
        "    }\n",
        "\n",
        "# Evaluate all models\n",
        "results = {}\n",
        "\n",
        "for model_name in models_config.keys():\n",
        "    results[model_name] = evaluate_model(\n",
        "        models[model_name],\n",
        "        tokenizers[model_name],\n",
        "        X_test,\n",
        "        y_test,\n",
        "        model_name\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comparison dataframe\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Model': list(results.keys()),\n",
        "    'Test Accuracy': [results[m]['accuracy'] for m in results.keys()],\n",
        "    'Inference Time (s)': [results[m]['inference_time'] for m in results.keys()],\n",
        "    'Parameters (M)': [metadata[m]['num_parameters']/1e6 for m in results.keys()],\n",
        "    'Model Size (MB)': [metadata[m]['model_size_mb'] for m in results.keys()]\n",
        "})\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODEL COMPARISON SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print(comparison_df.to_string(index=False))\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Visualize comparison\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Accuracy comparison\n",
        "ax1 = axes[0, 0]\n",
        "bars1 = ax1.bar(comparison_df['Model'], comparison_df['Test Accuracy'], \n",
        "                color=['#3498db', '#2ecc71', '#9b59b6'], edgecolor='black', linewidth=2)\n",
        "ax1.set_ylabel('Accuracy', fontsize=12)\n",
        "ax1.set_title('Test Accuracy Comparison', fontsize=14, fontweight='bold')\n",
        "ax1.set_ylim([0, 1])\n",
        "ax1.grid(True, alpha=0.3, axis='y')\n",
        "for i, v in enumerate(comparison_df['Test Accuracy']):\n",
        "    ax1.text(i, v + 0.02, f'{v:.4f}', ha='center', fontweight='bold')\n",
        "\n",
        "# Inference time comparison\n",
        "ax2 = axes[0, 1]\n",
        "bars2 = ax2.bar(comparison_df['Model'], comparison_df['Inference Time (s)'], \n",
        "                color=['#e74c3c', '#f39c12', '#e67e22'], edgecolor='black', linewidth=2)\n",
        "ax2.set_ylabel('Time (seconds)', fontsize=12)\n",
        "ax2.set_title('Inference Time Comparison', fontsize=14, fontweight='bold')\n",
        "ax2.grid(True, alpha=0.3, axis='y')\n",
        "for i, v in enumerate(comparison_df['Inference Time (s)']):\n",
        "    ax2.text(i, v + 0.5, f'{v:.2f}s', ha='center', fontweight='bold')\n",
        "\n",
        "# Model size comparison\n",
        "ax3 = axes[1, 0]\n",
        "bars3 = ax3.bar(comparison_df['Model'], comparison_df['Model Size (MB)'], \n",
        "                color=['#16a085', '#27ae60', '#2980b9'], edgecolor='black', linewidth=2)\n",
        "ax3.set_ylabel('Size (MB)', fontsize=12)\n",
        "ax3.set_title('Model Size Comparison', fontsize=14, fontweight='bold')\n",
        "ax3.grid(True, alpha=0.3, axis='y')\n",
        "for i, v in enumerate(comparison_df['Model Size (MB)']):\n",
        "    ax3.text(i, v + 5, f'{v:.1f} MB', ha='center', fontweight='bold')\n",
        "\n",
        "# Parameters comparison\n",
        "ax4 = axes[1, 1]\n",
        "bars4 = ax4.bar(comparison_df['Model'], comparison_df['Parameters (M)'], \n",
        "                color=['#8e44ad', '#c0392b', '#d35400'], edgecolor='black', linewidth=2)\n",
        "ax4.set_ylabel('Parameters (millions)', fontsize=12)\n",
        "ax4.set_title('Model Parameters Comparison', fontsize=14, fontweight='bold')\n",
        "ax4.grid(True, alpha=0.3, axis='y')\n",
        "for i, v in enumerate(comparison_df['Parameters (M)']):\n",
        "    ax4.text(i, v + 2, f'{v:.1f}M', ha='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Confusion Matrices - Side by Side\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create confusion matrices for all models\n",
        "fig, axes = plt.subplots(1, 3, figsize=(24, 7))\n",
        "\n",
        "colors = ['Blues', 'Greens', 'Purples']\n",
        "\n",
        "for idx, (model_name, color) in enumerate(zip(results.keys(), colors)):\n",
        "    cm = confusion_matrix(y_test, results[model_name]['predictions'])\n",
        "    \n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap=color, ax=axes[idx],\n",
        "                xticklabels=emotion_labels, yticklabels=emotion_labels)\n",
        "    axes[idx].set_xlabel('Predicted Label', fontsize=11)\n",
        "    axes[idx].set_ylabel('True Label', fontsize=11)\n",
        "    axes[idx].set_title(f'{model_name} Model - Confusion Matrix\\nAccuracy: {results[model_name][\"accuracy\"]:.4f}', \n",
        "                       fontsize=12, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù Classification Reports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print classification reports for all models\n",
        "for model_name in results.keys():\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"{model_name} MODEL - CLASSIFICATION REPORT\")\n",
        "    print(\"=\"*60)\n",
        "    print(classification_report(y_test, results[model_name]['predictions'], \n",
        "                               target_names=emotion_labels))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Per-Class Performance Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get per-class metrics for all models\n",
        "metrics_by_model = {}\n",
        "\n",
        "for model_name in results.keys():\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        y_test, \n",
        "        results[model_name]['predictions'], \n",
        "        average=None, \n",
        "        labels=list(range(6))\n",
        "    )\n",
        "    metrics_by_model[model_name] = {'precision': precision, 'recall': recall, 'f1': f1}\n",
        "\n",
        "# Create comparison dataframe\n",
        "emotion_comparison = pd.DataFrame({\n",
        "    'Emotion': emotion_labels,\n",
        "    'BERT F1': metrics_by_model['BERT']['f1'],\n",
        "    'RoBERTa F1': metrics_by_model['RoBERTa']['f1'],\n",
        "    'ELECTRA F1': metrics_by_model['ELECTRA']['f1']\n",
        "})\n",
        "\n",
        "print(\"\\nPer-Class F1-Score Comparison:\")\n",
        "print(emotion_comparison.to_string(index=False))\n",
        "\n",
        "# Visualize F1-scores comparison\n",
        "fig, ax = plt.subplots(figsize=(14, 7))\n",
        "\n",
        "x = np.arange(len(emotion_labels))\n",
        "width = 0.25\n",
        "\n",
        "bars1 = ax.bar(x - width, metrics_by_model['BERT']['f1'], width, \n",
        "               label='BERT', color='#3498db', edgecolor='black')\n",
        "bars2 = ax.bar(x, metrics_by_model['RoBERTa']['f1'], width, \n",
        "               label='RoBERTa', color='#2ecc71', edgecolor='black')\n",
        "bars3 = ax.bar(x + width, metrics_by_model['ELECTRA']['f1'], width, \n",
        "               label='ELECTRA', color='#9b59b6', edgecolor='black')\n",
        "\n",
        "ax.set_xlabel('Emotion', fontsize=12)\n",
        "ax.set_ylabel('F1-Score', fontsize=12)\n",
        "ax.set_title('F1-Score Comparison by Emotion Class', fontsize=14, fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(emotion_labels)\n",
        "ax.legend(fontsize=11)\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Add value labels on bars\n",
        "for bars in [bars1, bars2, bars3]:\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üèÜ Final Verdict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Determine best model\n",
        "best_model = max(results.keys(), key=lambda k: results[k]['accuracy'])\n",
        "best_accuracy = results[best_model]['accuracy']\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üèÜ FINAL VERDICT\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nü•á Best Model: {best_model}\")\n",
        "print(f\"   Test Accuracy: {best_accuracy:.4f} ({best_accuracy*100:.2f}%)\")\n",
        "print(f\"   Inference Time: {results[best_model]['inference_time']:.2f} seconds\")\n",
        "print(f\"   Model Size: {metadata[best_model]['model_size_mb']:.2f} MB\")\n",
        "print(f\"   Parameters: {metadata[best_model]['num_parameters']:,}\")\n",
        "\n",
        "print(\"\\nüìä All Models Summary:\")\n",
        "for model_name in results.keys():\n",
        "    icon = \"ü•á\" if model_name == best_model else \"  \"\n",
        "    print(f\"{icon} {model_name:10s}: {results[model_name]['accuracy']:.4f} accuracy | \"\n",
        "          f\"{results[model_name]['inference_time']:.2f}s inference | \"\n",
        "          f\"{metadata[model_name]['model_size_mb']:.1f} MB\")\n",
        "\n",
        "print(\"\\nüí° Key Insights:\")\n",
        "print(\"   - All transformer models significantly outperform traditional RNN architectures\")\n",
        "print(\"   - ELECTRA offers excellent sample efficiency with discriminative pre-training\")\n",
        "print(\"   - RoBERTa benefits from improved pre-training over BERT\")\n",
        "print(\"   - Model size and inference time are comparable across all three models\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üíæ Save Predictions\n",
        "\n",
        "Save predictions from the best model to CSV for submission.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create predictions DataFrame for best model\n",
        "predictions_df = pd.DataFrame({\n",
        "    'text': X_test.values,\n",
        "    'true_label': y_test.values,\n",
        "    'predicted_label': results[best_model]['predictions'],\n",
        "    'true_emotion': [emotion_labels[i] for i in y_test.values],\n",
        "    'predicted_emotion': [emotion_labels[i] for i in results[best_model]['predictions']]\n",
        "})\n",
        "\n",
        "# Save to CSV\n",
        "output_path = f'./data/test_predictions_{best_model.lower()}.csv'\n",
        "predictions_df.to_csv(output_path, index=False)\n",
        "print(f\"\\n‚úÖ Predictions saved to: {output_path}\")\n",
        "print(f\"   Total predictions: {len(predictions_df)}\")\n",
        "print(f\"   Accuracy: {best_accuracy:.4f}\")\n",
        "\n",
        "# Show sample predictions\n",
        "print(f\"\\nSample predictions:\")\n",
        "print(predictions_df[['text', 'true_emotion', 'predicted_emotion']].head(10).to_string(index=False))\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
